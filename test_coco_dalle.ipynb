{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from random import choice\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "# torch\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# vision imports\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "# dalle related classes and utils\n",
    "\n",
    "from dalle_pytorch import OpenAIDiscreteVAE, DiscreteVAE, DALLE\n",
    "from dalle_pytorch.simple_tokenizer import tokenize, tokenizer, VOCAB_SIZE\n",
    "\n",
    "from torchvision.datasets import CocoCaptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device 6,7 : True\n"
     ]
    }
   ],
   "source": [
    "gpu_id = '6,7'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"GPU device {} : {}\".format(gpu_id, use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# group = parser.add_mutually_exclusive_group(required = False)\n",
    "\n",
    "# group.add_argument('--vae_path', type = str,\n",
    "#                     help='path to your trained discrete VAE')\n",
    "\n",
    "# group.add_argument('--dalle_path', type = str,\n",
    "#                     help='path to your partially trained DALL-E')\n",
    "\n",
    "# parser.add_argument('--image_text_folder', type = str, required = True,\n",
    "#                     help='path to your folder of images and text for learning the DALL-E')\n",
    "# parser.add_argument('--resume', action='store_true')\n",
    "\n",
    "class Parser(object):\n",
    "    def __init__(self, vae_path, dalle_path, image_folder, anno_folder, resume):\n",
    "        self.vae_path = vae_path\n",
    "        self.dalle_path = dalle_path\n",
    "        self.image_folder = image_folder\n",
    "        self.anno_folder = anno_folder\n",
    "        self.resume = resume\n",
    "\n",
    "args = Parser('./vae-final.pt', '', '/vision/7052107/Dalle/coco/train2017/', '/vision/7052107/Dalle/coco/annotations/captions_train2017.json', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# constants\n",
    "VAE_PATH = args.vae_path\n",
    "DALLE_PATH = args.dalle_path\n",
    "IMAGE_PATH = args.image_folder\n",
    "ANNO_PATH = args.anno_folder\n",
    "RESUME = args.resume\n",
    "epoch_start = 1\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 30\n",
    "LEARNING_RATE = 1e-3\n",
    "GRAD_CLIP_NORM = 0.5\n",
    "\n",
    "MODEL_DIM = 512\n",
    "TEXT_SEQ_LEN = 256 # token num\n",
    "DEPTH = 4\n",
    "HEADS = 8\n",
    "DIM_HEAD = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=2.89s)\n",
      "creating index...\n",
      "index created!\n",
      "118287 image-text pairs found for training\n"
     ]
    }
   ],
   "source": [
    "if RESUME:\n",
    "    dalle_path = Path(DALLE_PATH)\n",
    "    assert dalle_path.exists(), 'DALL-E model file does not exist'\n",
    "\n",
    "    loaded_obj = torch.load(str(dalle_path))\n",
    "\n",
    "    dalle_params, vae_params, weights = loaded_obj['hparams'], loaded_obj['vae_params'], loaded_obj['weights']\n",
    "\n",
    "    vae = DiscreteVAE(**vae_params)\n",
    "#     print(dalle_params)\n",
    "#     dalle_params = dict(\n",
    "#         vae = vae,\n",
    "#         **dalle_params\n",
    "#     )\n",
    "\n",
    "    IMAGE_SIZE = vae_params['image_size']\n",
    "\n",
    "else:\n",
    "    if exists(VAE_PATH):\n",
    "        vae_path = Path(VAE_PATH)\n",
    "        assert vae_path.exists(), 'VAE model file does not exist'\n",
    "\n",
    "        loaded_obj = torch.load(str(vae_path))\n",
    "\n",
    "        vae_params, weights = loaded_obj['hparams'], loaded_obj['weights']\n",
    "\n",
    "        vae = DiscreteVAE(**vae_params)\n",
    "        vae.load_state_dict(weights)\n",
    "    else:\n",
    "        print('using OpenAIs pretrained VAE for encoding images to tokens')\n",
    "        vae_params = None\n",
    "\n",
    "        vae = OpenAIDiscreteVAE()\n",
    "\n",
    "    IMAGE_SIZE = vae.image_size\n",
    "\n",
    "    dalle_params = dict(\n",
    "        vae = vae,\n",
    "        num_text_tokens = VOCAB_SIZE,\n",
    "        text_seq_len = TEXT_SEQ_LEN,\n",
    "        dim = MODEL_DIM,\n",
    "        depth = DEPTH,\n",
    "        heads = HEADS,\n",
    "        dim_head = DIM_HEAD\n",
    "    )\n",
    "\n",
    "# helpers\n",
    "\n",
    "def save_model(path):\n",
    "    save_obj = {\n",
    "        'hparams': dalle_params,\n",
    "        'vae_params': vae_params,\n",
    "        'weights': dalle.module.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(save_obj, path)\n",
    "\n",
    "# create dataset and dataloader\n",
    "\n",
    "compose = T.Compose([T.Resize(IMAGE_SIZE),\n",
    "                     T.CenterCrop(IMAGE_SIZE),\n",
    "                     T.ToTensor(),])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "ds = CocoCaptions(root=IMAGE_PATH, annFile=ANNO_PATH, transform=compose)\n",
    "dl = DataLoader(ds, BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "assert len(ds) > 0, 'dataset is empty'\n",
    "print(f'{len(ds)} image-text pairs found for training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize DALL-E\n",
    "\n",
    "dalle = DALLE(**dalle_params)\n",
    "\n",
    "if RESUME:\n",
    "    dalle.load_state_dict(weights)\n",
    "\n",
    "\n",
    "dalle = torch.nn.DataParallel(dalle).cuda()\n",
    "\n",
    "# optimizer\n",
    "\n",
    "opt = Adam(dalle.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "# experiment tracker\n",
    "\n",
    "# import wandb\n",
    "\n",
    "# wandb.config.depth = DEPTH\n",
    "# wandb.config.heads = HEADS\n",
    "# wandb.config.dim_head = DIM_HEAD\n",
    "\n",
    "# wandb.init(project = 'dalle_train_transformer', resume = RESUME)\n",
    "\n",
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anaconda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 loss - 18.287561416625977\n",
      "1 10 loss - 2.4109811782836914\n",
      "1 20 loss - 2.108689308166504\n",
      "1 30 loss - 2.3646788597106934\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_start, EPOCHS):\n",
    "    for i, (images, text) in enumerate(dl):\n",
    "        images = torch.stack(images)\n",
    "        text_list = []\n",
    "        for descriptions in text:\n",
    "            descriptions = list(filter(lambda t: len(t) > 0, descriptions))\n",
    "            description= choice(descriptions)\n",
    "            text_list.append(description)\n",
    "        text = tokenize(text_list).squeeze(0)\n",
    "        mask = text != 0\n",
    "        text, images, mask = map(lambda t: t.cuda(), (text, images, mask))\n",
    "        loss = dalle(text, images, mask = mask, return_loss = True)\n",
    "        loss = torch.sum(loss)\n",
    "        loss.backward()\n",
    "#         clip_grad_norm_(dalle.parameters(), GRAD_CLIP_NORM)\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        log = {}\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(epoch, i, f'loss - {loss.item()}')\n",
    "\n",
    "#             log = {\n",
    "#                 **log,\n",
    "#                 'epoch': epoch,\n",
    "#                 'iter': i,\n",
    "#                 'loss': loss.item()\n",
    "#             }\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            sample_text = text[:1]\n",
    "            token_list = sample_text.masked_select(sample_text != 0).tolist()\n",
    "            decoded_text = tokenizer.decode(token_list)\n",
    "\n",
    "            image = dalle.module.generate_images(\n",
    "                text[:1],\n",
    "                mask = mask[:1],\n",
    "                filter_thres = 0.5   # topk sampling at 0.9\n",
    "            )\n",
    "\n",
    "            save_model(f'./dalle_coco.pt')\n",
    "#             wandb.save(f'./dalle.pt')\n",
    "\n",
    "#             log = {\n",
    "#                 **log,\n",
    "#                 'image': wandb.Image(image, caption = decoded_text)\n",
    "#             }\n",
    "\n",
    "#         wandb.log(log)\n",
    "\n",
    "save_model(f'./dalle-final_coco.pt')\n",
    "# wandb.save('./dalle-final.pt')\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vision",
   "language": "python",
   "name": "vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
